{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249173f5-697e-4952-80b0-3afac10870d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages to use during training\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer\n",
    "import tensorflow as tf\n",
    "from transformers import TFBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37479bc5-ffc8-41f9-9dcb-a60750c875be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"IMDB-Movie-Reviews/supervised.csv\") \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7638ca8-f131-4995-9596-055fcf1d0153",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[df.fold == 'train'][:1000]\n",
    "test = df[df.fold == 'test']\n",
    "\n",
    "\n",
    "print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e3ddc2-4928-4e6e-96e0-300861f32910",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d44baa-a713-439b-9cb1-b3fda804bdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for encoding of the text reviews\n",
    "def convert_example_to_feature(review):\n",
    "  return tokenizer.encode_plus(review,\n",
    "                add_special_tokens = True, # add [CLS], [SEP]\n",
    "                max_length = max_length, # max length of the text that can go to BERT\n",
    "                pad_to_max_length = True, # add [PAD] tokens\n",
    "                return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299c5fd3-195d-4732-ba61-770f65da6e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4508cc5e-dce4-4dec-bc2a-c74302cf99c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function to format the model output \n",
    "def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n",
    "  return {\n",
    "      \"input_ids\": input_ids,\n",
    "      \"token_type_ids\": token_type_ids,\n",
    "      \"attention_mask\": attention_masks,\n",
    "  }, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea40f40-850e-430f-865c-25b78765b2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting all the functions together to complete the tokenization process\n",
    "label_map = {'pos': 1, 'neg': 0}\n",
    "\n",
    "def encode_examples(ds, limit=-1):\n",
    "  input_ids_list = []\n",
    "  token_type_ids_list = []\n",
    "  attention_mask_list = []\n",
    "  label_list = []\n",
    "  if (limit > 0):\n",
    "      ds = ds.take(limit)\n",
    "  for path, label in zip(df['path'], df['label']):\n",
    "    review = open(\"IMDB-Movie-Reviews/\"+path, \"r\").read()\n",
    "    bert_input = convert_example_to_feature(review)\n",
    "    input_ids_list.append(bert_input['input_ids'])\n",
    "    token_type_ids_list.append(bert_input['token_type_ids'])\n",
    "    attention_mask_list.append(bert_input['attention_mask'])\n",
    "\n",
    "\n",
    "    numeric_label = label_map[label]  # Convert string label using label_map\n",
    "    label_list.append([numeric_label])\n",
    "    # label_list.append([label])\n",
    "  return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b1cff6-8d52-4683-86ec-e75bb292b9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train dataset\n",
    "ds_train_encoded = encode_examples(train).shuffle(10000).batch(batch_size)\n",
    "# test dataset\n",
    "ds_test_encoded = encode_examples(test).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da67659a-254c-48f8-94c0-dd4fbe659f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommended learning rate for Adam 5e-5, 3e-5, 2e-5\n",
    "learning_rate = 2e-5\n",
    "# we will do just 1 epoch, though multiple epochs might be better as long as we will not overfit the model\n",
    "number_of_epochs = 1\n",
    "# model initialization\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "# choosing Adam optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n",
    "# we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de53997",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "      ds_train_encoded,\n",
    "      batch_size=32,\n",
    "      epochs=number_of_epochs)\n",
    "\n",
    "model.save(\"model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c9f513",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e99c54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "model.load_weights(\"model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9015f4-796a-42f2-97df-927f96d0c46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"this movie was so good, love the acting and soundtrack\"\n",
    "\n",
    "predict_input = tokenizer.encode(test_sentence,\n",
    "\n",
    "truncation=True,\n",
    "\n",
    "padding=True,\n",
    "\n",
    "return_tensors=\"tf\")\n",
    "\n",
    "tf_output = model.predict(predict_input)[0]\n",
    "tf_prediction = tf.nn.softmax(tf_output, axis=1)\n",
    "labels = ['Negative','Positive'] #(0:negative, 1:positive)\n",
    "label = tf.argmax(tf_prediction, axis=1)\n",
    "label = label.numpy()\n",
    "print(labels[label[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406faab7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
